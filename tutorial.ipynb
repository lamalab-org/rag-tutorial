{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d879f24a0a883238",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Einf√ºhrung: Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e11155c474a6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Dieses Notebook demonstriert die Grundstruktur eines Retrieval-Augmented Generation (RAG) Systems.  \n",
    "RAG kombiniert die St√§rken von Retrieval- und Generierungsmodellen, um pr√§zise Antworten auf Fragen zu liefern, die auf spezifischen Dokumenten basieren.\n",
    "Retrieval-Modelle durchsuchen gro√üe Dokumentensammlungen, um relevante Informationen zu finden, w√§hrend Generierungsmodelle diese Informationen nutzen, um koh√§rente Antworten zu formulieren.\n",
    "\n",
    "Praktisch kann ein solches System genutzt werden, um Fragen zu beantworten, die auf einem bestimmten Dokument basieren, ohne dass das zugrunde liegende Sprachmodell neu trainiert werden muss.\n",
    "RAG, mit einigen weiteren Tricks, ist damit die Grundlage f√ºr Systeme wie [PaperQA](https://arxiv.org/abs/2409.13740) oder [ScholarQA](https://scholarqa.allen.ai/chat) die Fragen basierend auf wissenschaftlichen Artikeln beantworten k√∂nnen.\n",
    "Da Retrieval verwendet wird, kann immer auch eine Referenz zu den Quellen gegeben werden, die f√ºr die Antwort verwendet wurden.\n",
    "\n",
    "## Aufbau eines RAG-Systems\n",
    "Prinzipiell besteht ein RAG-System aus zwei Hauptkomponenten:\n",
    "1. **Retrieval**: Hierbei werden relevante Abschnitte aus einem Dokument oder einer Sammlung von Dokumenten abgerufen, die f√ºr die Beantwortung der gestellten Frage n√ºtzlich sein k√∂nnten. Hierf√ºr werden Dokumente in kleine Abschnitte (\"Chunks\") unterteilt und in einem Vektor-Datenbankindex gespeichert.  Der Index wird erstellt, indem die Abschnitte in Vektoren umgewandelt werden, die dann in der Datenbank gespeichert werden. Wenn eine Frage gestellt wird, wird der Index durchsucht, um die relevantesten Abschnitte zu finden.\n",
    "2. **Generierung**: Basierend auf den abgerufenen Informationen generiert ein Sprachmodell eine Antwort auf die gestellte Frage.\n",
    "\n",
    "\n",
    "## Ziel des Notebooks \n",
    "Fragen zu benutzerdefinierten Dokumenten beantworten ‚Äì **ohne das Modell neu zu trainieren**.\n",
    "\n",
    "## Voraussetzungen\n",
    "- Python 3.8 oder h√∂her\n",
    "- Ein Ordner mit PDF-Dokumenten, die du verwenden m√∂chtest (z.B. `./data`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64eefca87d2f2a9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:35:12.046806Z",
     "start_time": "2025-06-04T12:35:09.887152Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Ben√∂tigten Pakete installieren ---\n",
    "\n",
    "!pip install -q pymupdf        # F√ºr PDF-Text-Extraktion\n",
    "!pip install -q numpy          # F√ºr Cosine Similarity & Vektorberechnungen\n",
    "!pip install -q litellm        # F√ºr Zugriff auf Embedding- & Sprachmodelle via API\n",
    "!pip install -q langchain      # F√ºr die Verwaltung von Embeddings und Modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "518db8ed48a1491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:38:03.209129Z",
     "start_time": "2025-06-04T12:38:02.804395Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import os                 \n",
    "import fitz # PyMuPDF          \n",
    "import numpy as np        \n",
    "import litellm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from typing import List, Tuple\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219db1c",
   "metadata": {},
   "source": [
    "Zunachst laden wir die Umgebungsvariablen aus der `.env`-Datei, die API-Schl√ºssel und andere Konfigurationen enthalten sollte.\n",
    "Es ist zu empfehlen, dass solche API-Schl√ºssel nicht direkt im Code stehen, sondern in einer `.env`-Datei gespeichert werden.\n",
    "\n",
    "Dafur erstellst du im gleichen Verzeichnis wie dieses Skript eine Datei mit dem Namen `.env` und f√ºgst dort deine API-Schl√ºssel ein, z.B.:\n",
    "```plaintext\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "```\n",
    "\n",
    "Alternativ kann auch Groq oder ein anderer Provider verwendet werden. Eine komplette √úbersicht gibt es auf https://docs.litellm.ai/docs/providers.  \n",
    "Cohere bietet kostenlose API Keys mit einer Token-Begrenzung an https://docs.cohere.com/v2/docs/rate-limits \n",
    "\n",
    "`load_dotenv()` l√§dt die Umgebungsvariablen aus der `.env`-Datei, sodass wir sie im Code verwenden k√∂nnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ad53a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()  # L√§dt Umgebungsvariablen aus .env-Datei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a13782576ab6e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üìÑ Schritt 1: PDF-Dokumente einlesen\n",
    "\n",
    "Zuerst werden PDF-Dateien mit dem Python-Paket `fitz` (PyMuPDF) in reinen Text umgewandelt. Daf√ºr definieren wir eine Funktion `extract_text_from_pdfs`, die alle PDF-Dateien in einem angegebenen Verzeichnis liest und den Text extrahiert.\n",
    "Die Funktion ist unvollst√§ndig und muss noch implementiert werden.  Hinweise zum Implementieren der Funktion findest du in den Kommentaren im Code sowie in der [Dokumentation von `fitz` (PyMuPDF)](https://pymupdf.readthedocs.io/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d16c7c78334ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:35:23.890601Z",
     "start_time": "2025-06-04T12:35:23.746594Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf_folder(pdf_folder_path):\n",
    "    all_text = \"\"\n",
    "    \n",
    "    # √úber alle PDF-Dateien im Ordner iterieren\n",
    "    for filename in os.listdir(pdf_folder_path):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            file_path = os.path.join(pdf_folder_path, filename)\n",
    "            # TODO: √ñffne die PDF-Datei mit fitz.open(file_path) als doc\n",
    "            for page in doc:\n",
    "                # TODO: Verwende .get_text(), um Text zu extrahieren und zu all_text hinzuzuf√ºgen\n",
    "                pass\n",
    "    \n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c7563",
   "metadata": {},
   "source": [
    "Nun k√∂nnen wir die Funktion zum Extrahieren von Text aus PDF-Dateien testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40221fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aufruf der Extraktion ---\n",
    "pdf_folder = \"...\"  # TODO: Gib den Ordnerpfad mit den PDFs an\n",
    "raw_text = extract_text_from_pdf_folder(pdf_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137fac1b",
   "metadata": {},
   "source": [
    "Und uns den extrahierten Text anzeigen lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_text[1000]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb4f58f9e45d74",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ‚úÇÔ∏è Schritt 2: Text in Chunks zerlegen\n",
    "\n",
    "Der extrahierte Text wird nun in kleinere, √ºberlappende Abschnitte (*Chunks*) aufgeteilt.  \n",
    "Hierfur verwenden wir eine Methode die `RecursiveCharacterTextSplitter` genannt wird. Diese Methode teilt den Text in kleinere Abschnitte auf, die f√ºr die sp√§tere Verarbeitung durch das Retrieval-Modell geeignet sind. Die Chunks werden so erstellt, dass sie eine maximale L√§nge haben und √ºberlappende Teile enthalten, um sicherzustellen, dass wichtige Informationen nicht verloren gehen. Die Abschnitte werden hierbei erstellt indem der Text an einer definierten Liste von Trennzeichen (wie Abs√§tzen oder S√§tzen) aufgeteilt wird. Diese Liste wird durchgegangen bis der Text in kleinere Abschnitte zerlegt ist, die eine maximale L√§nge nicht √ºberschreiten.\n",
    "Mehr Informationen dazu k√∂nnen in der [LangChain Dokumentation](https://python.langchain.com/docs/how_to/recursive_text_splitter/) gefunden werden.\n",
    "\n",
    "Diese Einheiten k√∂nnen sp√§ter effizient eingebettet und durchsucht werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e01284dfb4ac7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-04T12:36:32.805386Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Text in √ºberlappende Chunks aufteilen ---\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=___,       # TODO: W√§hle sinnvolle Chunk-Gr√∂√üe\n",
    "    chunk_overlap=___     # TODO: W√§hle √úberlappung\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text()  # TODO: Gib den zu chunkenden Text ein\n",
    "print(f\"{len(chunks)} Chunks erstellt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57895240eb1851",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üî¢ Schritt 3: Chunks embedden\n",
    "\n",
    "Nun werden die erzeugten Chunks in numerische Vektoren (*Embeddings*) umgewandelt.  \n",
    "Diese Vektoren repr√§sentieren die semantische Bedeutung der Chunks und erm√∂glichen es, √§hnliche Chunks zu finden. Texte mit √§hnlicher Bedeutung werden in der Vektor-Datenbank nahe beieinander liegen. \n",
    "Text in Vektoren umzuwandeln, wird als \"Embedding\" bezeichnet und ist der erste Schritt in Sprachmodellen, um Text in eine Form zu bringen, die von Computern verarbeitet werden kann. Hier verwenden wir die Embeddings allerdings auch um die Chunks in einer Vektor-Datenbank zu speichern, damit sie sp√§ter f√ºr die Retrieval-Komponente des RAG-Systems verwendet werden k√∂nnen.\n",
    "\n",
    "\n",
    "Dies erfolgt mithilfe der OpenAI API √ºber `LiteLLM`. Es k√∂nnen auch andere Embedding-Modelle verwendet werden, die in der Lage sind, Text in Vektoren umzuwandeln. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e645a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_with_litellm(text: str, model: str = 'text-embedding-3-small') -> List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the given text using the LiteLLM API.\n",
    "    \"\"\"\n",
    "    response = litellm.embedding(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "    return response[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f75b71",
   "metadata": {},
   "source": [
    "Das k√∂nnen wir testen indem wir verschiedene Texte einbetten und die Vektoren vergleichen. Um  Vektoren zu vergleichen, k√∂nnen wir den Winkel zwischen ihnen berechnen. Ein kleiner Winkel bedeutet, dass die Vektoren √§hnlich sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e626fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Berechnet die Cosine Similarity zwischen zwei Vektoren.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_a = np.linalg.norm(vec1)\n",
    "    norm_b = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6eddbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a = \"Chemie\"\n",
    "text_b = \"Chemie ist die Wissenschaft von Stoffen und deren Umwandlungen.\"\n",
    "text_c = \"Mathematik ist die Wissenschaft von Zahlen und Formen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cabb4970",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_a_b = cosine_similarity(\n",
    "    np.array(embed_text_with_litellm(text_a)),\n",
    "    np.array(embed_text_with_litellm(text_b))\n",
    ")\n",
    "\n",
    "cosine_a_c = cosine_similarity(\n",
    "    np.array(embed_text_with_litellm(text_a)),\n",
    "    np.array(embed_text_with_litellm(text_c))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b85c8350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity zwischen 'Chemie' und 'Chemie ist die Wissenschaft von Stoffen und deren Umwandlungen.': 0.7127\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cosine Similarity zwischen '{text_a}' und '{text_b}': {cosine_a_b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb8307c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity zwischen 'Chemie' und 'Mathematik ist die Wissenschaft von Zahlen und Formen.': 0.3313\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cosine Similarity zwischen '{text_a}' und '{text_c}': {cosine_a_c:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae3d7e",
   "metadata": {},
   "source": [
    "Nun k√∂nnen wir die Chunks in Embeddings umwandeln und die Cosine Similarity berechnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29694ebb69205442",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Beispielkonfiguration (OpenAI, austauschbar)\n",
    "litellm_model = \"openai/embedding-3-small\"\n",
    "\n",
    "# Liste von Embeddings vorbereiten\n",
    "embeddings = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    embeddings.append(embed_text_with_litellm(chunk, model=litellm_model))\n",
    "\n",
    "print(f\"{len(embeddings)} Embeddings erstellt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afec90df299ef6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üß† Schritt 4: Erstellung eines Vektorstores\n",
    "\n",
    "Die Embeddings und ihre zugeh√∂rigen Textabschnitte werden in einem einfachen Vektorstore gespeichert.  \n",
    "Dazu erstellen wir eine **Liste von Paaren** bestehend aus:\n",
    "\n",
    "- einem Embedding  \n",
    "- dem zugeh√∂rigen Textabschnitt\n",
    "\n",
    "‚û°Ô∏è Dies erlaubt sp√§ter eine **schnelle semantische Suche**.\n",
    "\n",
    "Die `zip`-Funktion in Python kann verwendet werden, um zwei Listen zu kombinieren, sodass jedes Element der ersten Liste mit dem entsprechenden Element der zweiten Liste gepaart wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35abbf069abad08f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorstore = list(zip(___, ___)) # TODO: Embeddings + zugeh√∂rige Text Chunk definieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1347e831917212a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üîç Schritt 5: Retrieval der relevanten Textabschnitte\n",
    "\n",
    "Um zu einer Nutzeranfrage passende Textstellen zu finden, berechnen wir die **Cosine Similarity**  \n",
    "zwischen dem Embedding der Frage und allen gespeicherten Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666614c304af9fd0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nun sollen die **k √§hnlichsten Chunks** zur Nutzeranfrage (`query`) gefunden und zur√ºckgegeben werden.\n",
    "\n",
    "Hierzu definieren wir die Funktion `retrieve_top_k`. Diese Funktion nimmt die Nutzeranfrage (`query`) und die Anzahl der gew√ºnschten Ergebnisse (`k`) als Eingabeparameter. Sie berechnet die Cosine Similarity zwischen dem Embedding der Anfrage und den gespeicherten Embeddings und gibt die `k` √§hnlichsten Chunks zur√ºck.\n",
    "\n",
    "Die Funktion `retrieve_top_k` wird wie folgt implementiert:\n",
    "\n",
    "1. Berechnung des Embeddings der Anfrage (`query_embedding`).\n",
    "2. Berechnung der Cosine Similarity zwischen dem `query_embedding` und allen im Vektorstore `vectorstore` gespeicherten Embeddings mittels `cosine_similarity`.\n",
    "3. Sortierung der Ergebnisse nach der Cosine Similarity in absteigender Reihenfolge. Die Cosine Similarity kann zwischen -1 und 1 liegen, wobei 1 die h√∂chste √Ñhnlichkeit bedeutet. Deshalb sortieren wir die Ergebnisse in absteigender Reihenfolge.\n",
    "4. R√ºckgabe der `k` √§hnlichsten Chunks und ihrer Cosine Similarity-Werte. Hierbei m√ºssen wir darauf achten, dass in manchen F√§llen `k` gr√∂√üer sein kann als die Anzahl der verf√ºgbaren Chunks. In diesem Fall sollten wir nur die verf√ºgbaren Chunks zur√ºckgeben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc00be26937037f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- √Ñhnlichste Chunks zur Nutzerfrage finden ---\n",
    "def retrieve_top_k(query: str, k: int = 3) -> List[str]:\n",
    "\n",
    "    query_embedding = np.array(embed_text_with_litellm(query, model=litellm_model))\n",
    "\n",
    "    # √Ñhnlichkeit mit allen gespeicherten Embeddings berechnen\n",
    "    scored_chunks = []\n",
    "    for embedding, text in vectorstore:\n",
    "        embedding = np.array(embedding)\n",
    "        score = cosine_similarity(___, ___) # TODO: Cosine Similarity korrekt aufrufen\n",
    "        scored_chunks.append((score, text))\n",
    "\n",
    "    # Chunks nach Score absteigend sortieren\n",
    "    scored_chunks = sorted(\n",
    "        ___,                      # TODO: Liste der Scoring-Ergebnisse einsetzen\n",
    "        key=lambda x: x[0],       # Sortiere nach dem ersten Element im Tupel = Score\n",
    "        reverse=True              # H√∂chste Scores zuerst\n",
    "    )\n",
    "\n",
    "    # Texte der Top-k Ergebnisse zur√ºckgeben\n",
    "    top_chunks = []\n",
    "\n",
    "    for i in range(min(___, len(___))):  # TODO: Ersetze beide ___ mit der gew√ºnschten Anzahl an Ergebnissen und der L√§nge der Liste scored_chunks\n",
    "        top_chunks.append(scored_chunks[i][1])  \n",
    "        \n",
    "    return top_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26238a794e42db",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üí¨ Schritt 6: Nutzeranfrage stellen und Antwort generieren\n",
    "\n",
    "Die Nutzerfrage wird zun√§chst ebenfalls in ein Embedding umgewandelt.  \n",
    "Danach werden die semantisch √§hnlichsten Chunks aus dem Vektorstore geladen.  \n",
    "Diese bilden den **Kontext**, den das Sprachmodell (z.B. GPT-4) verwendet, um eine Antwort zu generieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca042584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Nutzerfrage stellen ---\n",
    "query = \"...\"  # TODO: Gib hier deine Frage ein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68513e1e",
   "metadata": {},
   "source": [
    "Nun k√∂nnen wir die Top-k Chunks abrufen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf02733",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_chunks = retrieve_top_k(___, ___)  # TODO: Argumente der Abfragefunktion definieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddbb3b2",
   "metadata": {},
   "source": [
    "Um basierend auf der Literatur und Nutzerfrage eine Antwort zu generieren, kannst du ein Sprachmodell verwenden. \n",
    "Hiefur kombinieren wir die Top-k Chunks und senden zwei `Messages` an das Modell. \n",
    "Eine `Message`  ist der sogenannte `System Prompt`, der dem Modell Kontext gibt. Der `System Prompt` definiert die Rolle des Modells und gibt Anweisungen, wie es antworten soll. Er wird in der Regel einmalig zu Beginn der Konversation festgelegt und bleibt w√§hrend der gesamten Sitzung unver√§ndert.\n",
    "Die andere `Message` ist die `User Message`, die die eigentliche Nutzerfrage enth√§lt. In dieser `Message` wird das Modell aufgefordert, eine Antwort auf die gestellte Frage zu generieren. Hierf√ºr wird der Text der Top-k Chunks als Kontext hinzugef√ºgt, um dem Modell relevante Informationen zu liefern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4652a1a28155b2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Prompt vorbereiten ---\n",
    "retrieved_context = \"\\n\\n\".join(top_chunks)\n",
    "\n",
    "# --- Prompt + Frage an Sprachmodell √ºbergeben (z.B. GPT-4 via LiteLLM) ---\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-4\",  # TODO: Modell ggf. anpassen\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Beantworte Fragen basierend auf den folgenden Textausz√ºgen.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Textausz√ºge: {___}\\n\\nFrage: {___}\"} # TODO: √Ñhnliche Textausz√ºge und Nutzteranfrage definieren\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Antwort anzeigen ---\n",
    "print(\"Antwort:\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5251dc6b2ee3c0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In diesem Tutorial hast du Schritt f√ºr Schritt ein einfaches Retrieval-Augmented Generation (RAG) System aufgebaut.\n",
    "\n",
    "Du hast gelernt:\n",
    "\n",
    "- wie man Dokumente in Text umwandelt,\n",
    "- wie man diesen Text in verarbeitbare Chunks aufteilt,\n",
    "- wie man eigene Embeddings erzeugt,\n",
    "- und wie man eine semantische Suche selbst implementiert.\n",
    "\n",
    "Anschlie√üend konntest du mit Hilfe eines Sprachmodells Fragen zu beliebigen Dokumenten beantworten.\n",
    "\n",
    "üîß Dieses Grundger√ºst l√§sst sich nun beliebig erweitern ‚Äì z.B. mit:\n",
    "- Vektor-Datenbanken wie FAISS, Chroma oder Weaviate\n",
    "- lokalen Sprachmodellen (z.B. √ºber Ollama, Hugging Face)\n",
    "- anderen Datenquellen (z.B. HTML, CSV, Notizen, Mails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191cd34c",
   "metadata": {},
   "source": [
    "In der Praxis wird RAG h√§ufig in Kombination mit anderen Techniken verwendet, um die Pr√§zision und Relevanz der Antworten zu verbessern. \n",
    "Sehr hilfreich kann es sein das Sprachmodell mehrmals aufzurufen: zum Beispiel um viele Chunks zusammenzufassen und die Relevanz der Chunks zu bewerten, bevor die finale Antwort generiert wird. \n",
    "Sehr oft wird auch die Suche in Vektor-Datenbanken mit einer \"klassichen\" Textsuche kombiniert, um die Relevanz der Ergebnisse zu erh√∂hen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
