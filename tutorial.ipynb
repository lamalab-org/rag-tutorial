{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d879f24a0a883238",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Einf√ºhrung: Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e11155c474a6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Dieses Notebook demonstriert die Grundstruktur eines Retrieval-Augmented Generation (RAG) Systems.  \n",
    "RAG kombiniert die St√§rken von Retrieval- und Generierungsmodellen, um pr√§zise Antworten auf Fragen zu liefern, die auf spezifischen Dokumenten basieren.\n",
    "Retrieval-Modelle durchsuchen gro√üe Dokumentensammlungen, um relevante Informationen zu finden, w√§hrend Generierungsmodelle diese Informationen nutzen, um koh√§rente Antworten zu formulieren.\n",
    "\n",
    "Praktisch kann ein solches System genutzt werden, um Fragen zu beantworten, die auf einem bestimmten Dokument basieren, ohne dass das zugrunde liegende Sprachmodell neu trainiert werden muss.\n",
    "\n",
    "## Aufbau eines RAG-Systems\n",
    "Prinzipiell besteht ein RAG-System aus zwei Hauptkomponenten:\n",
    "1. **Retrieval**: Hierbei werden relevante Abschnitte aus einem Dokument oder einer Sammlung von Dokumenten abgerufen, die f√ºr die Beantwortung der gestellten Frage n√ºtzlich sein k√∂nnten. Hierf√ºr werden Dokumente in kleine Abschnitte (\"Chunks\") unterteilt und in einem Vektor-Datenbankindex gespeichert.  Der Index wird erstellt, indem die Abschnitte in Vektoren umgewandelt werden, die dann in der Datenbank gespeichert werden. Wenn eine Frage gestellt wird, wird der Index durchsucht, um die relevantesten Abschnitte zu finden.\n",
    "2. **Generierung**: Basierend auf den abgerufenen Informationen generiert ein Sprachmodell eine Antwort auf die gestellte Frage.\n",
    "\n",
    "\n",
    "## Ziel des Notebooks \n",
    "Fragen zu benutzerdefinierten Dokumenten beantworten ‚Äì **ohne das Modell neu zu trainieren**.\n",
    "\n",
    "## Voraussetzungen\n",
    "- Python 3.8 oder h√∂her\n",
    "- Ein Ordner mit PDF-Dokumenten, die Sie verwenden m√∂chten (z.B. `./data`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64eefca87d2f2a9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:35:12.046806Z",
     "start_time": "2025-06-04T12:35:09.887152Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Ben√∂tigten Pakete installieren ---\n",
    "\n",
    "!pip install -q pymupdf        # F√ºr PDF-Text-Extraktion\n",
    "!pip install -q numpy          # F√ºr Cosine Similarity & Vektorberechnungen\n",
    "!pip install -q litellm        # F√ºr Zugriff auf Embedding- & Sprachmodelle via API\n",
    "!pip install -q langchain      # F√ºr die Verwaltung von Embeddings und Modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518db8ed48a1491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:38:03.209129Z",
     "start_time": "2025-06-04T12:38:02.804395Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import os                 \n",
    "import fitz # PyMuPDF          \n",
    "import numpy as np        \n",
    "import litellm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0fab8996c6118f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- OpenAI API Key setzen (√ºber Umgebungsvariable) ---\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # TODO: Ersetze durch deinen eigenen API-Schl√ºssel. Dieser API-Key kann auf https://platform.openai.com/account/api-keys generiert werden. \n",
    "\n",
    "# Alternativ kann auch Groq oder ein anderer Provider verwendet werden. Eine komplette √úbersicht gibt es auf https://docs.litellm.ai/docs/providers.  \n",
    "# Cohere bietet kostenlose API Keys mit einer Token-Begrenzung an https://docs.cohere.com/v2/docs/rate-limits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a13782576ab6e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üìÑ Schritt 1: PDF-Dokumente einlesen\n",
    "\n",
    "Zuerst werden PDF-Dateien mit dem Python-Paket `fitz` (PyMuPDF) in reinen Text umgewandelt. Daf√ºr definieren wir eine Funktion `extract_text_from_pdfs`, die alle PDF-Dateien in einem angegebenen Verzeichnis liest und den Text extrahiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d16c7c78334ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:35:23.890601Z",
     "start_time": "2025-06-04T12:35:23.746594Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text from folder './pdfs', showing preview:\n",
      "\n",
      "Polymer Solubility Prediction Using Large\n",
      "Language Models\n",
      "Published as part of ACS Materials Letters special issue ‚ÄúMachine Learning for Materials Chemistry‚Äù.\n",
      "Sakshi Agarwal, Akhlak Mahmood, and Rampi Ramprasad*\n",
      "Cite This: ACS Materials Lett. 2025, 7, 2017‚àí2023\n",
      "Read Online\n",
      "ACCESS\n",
      "Metrics & More\n",
      "Article Recommendations\n",
      "*\n",
      "sƒ±\n",
      "Supporting Information\n",
      "ABSTRACT: Traditional approaches in polymer informatics often require labor-intensive data curation, time-\n",
      "consuming preprocessing such as fingerprinting, and choosing suitable learning algorithms. Large language models\n",
      "(LLMs) represent a compelling alternative by addressing these limitations with their inherent flexibility, ease of use,\n",
      "and scalability. In this study, we propose a novel approach utilizing fine-tuned LLMs to classify solvents and\n",
      "nonsolvents for polymers, a property critical to polymer synthesis, purification, and diverse applications. Our results\n",
      "show that fine-tuned GPT-3.5 achieves predictive performance comparable to or exc\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf_folder(pdf_folder_path):\n",
    "    all_text = \"\"\n",
    "    \n",
    "    # √úber alle PDF-Dateien im Ordner iterieren\n",
    "    for filename in os.listdir(pdf_folder_path):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            file_path = os.path.join(pdf_folder_path, filename)\n",
    "            # TODO: √ñffne die PDF-Datei mit fitz.open(file_path) als doc\n",
    "            for page in doc:\n",
    "                # TODO: Verwende .get_text(), um Text zu extrahieren und zu all_text hinzuzuf√ºgen\n",
    "                pass\n",
    "    \n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c7563",
   "metadata": {},
   "source": [
    "Nun k√∂nnen wir die Funktion zum Extrahieren von Text aus PDF-Dateien testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40221fa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_text_from_pdf_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Aufruf der Extraktion ---\u001b[39;00m\n\u001b[32m      2\u001b[39m pdf_folder = \u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# TODO: Gib den Ordnerpfad mit den PDFs an\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m raw_text = \u001b[43mextract_text_from_pdf_folder\u001b[49m(pdf_folder)\n",
      "\u001b[31mNameError\u001b[39m: name 'extract_text_from_pdf_folder' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Aufruf der Extraktion ---\n",
    "pdf_folder = \"...\"  # TODO: Gib den Ordnerpfad mit den PDFs an\n",
    "raw_text = extract_text_from_pdf_folder(pdf_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137fac1b",
   "metadata": {},
   "source": [
    "Und uns den extrahierten Text anzeigen lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_text[1000]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb4f58f9e45d74",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ‚úÇÔ∏è Schritt 2: Text in Chunks zerlegen\n",
    "\n",
    "Der extrahierte Text wird nun in kleinere, √ºberlappende Abschnitte (*Chunks*) aufgeteilt.  \n",
    "Hierfur verwenden wir eine Methode die `RecursiveCharacterTextSplitter` genannt wird. Diese Methode teilt den Text in kleinere Abschnitte auf, die f√ºr die sp√§tere Verarbeitung durch das Retrieval-Modell geeignet sind. Die Chunks werden so erstellt, dass sie eine maximale L√§nge haben und √ºberlappende Teile enthalten, um sicherzustellen, dass wichtige Informationen nicht verloren gehen. Die Abschnitte werden hierbei erstellt indem der Text an einer definierten Liste von Trennzeichen (wie Abs√§tzen oder S√§tzen) aufgeteilt wird. Diese Liste wird durchgegangen bis der Text in kleinere Abschnitte zerlegt ist, die eine maximale L√§nge nicht √ºberschreiten.\n",
    "Mehr Informationen dazu k√∂nnen in der [LangChain Dokumentation](https://python.langchain.com/docs/how_to/recursive_text_splitter/) gefunden werden.\n",
    "\n",
    "Diese Einheiten k√∂nnen sp√§ter effizient eingebettet und durchsucht werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e01284dfb4ac7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-04T12:36:32.805386Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260 chunks created.\n"
     ]
    }
   ],
   "source": [
    "# --- Text in √ºberlappende Chunks aufteilen ---\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=___,       # TODO: W√§hle sinnvolle Chunk-Gr√∂√üe\n",
    "    chunk_overlap=___     # TODO: W√§hle √úberlappung\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text()  # TODO: Gib den zu chunkenden Text ein\n",
    "print(f\"{len(chunks)} Chunks erstellt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57895240eb1851",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üî¢ Schritt 3: Chunks embedden\n",
    "\n",
    "Nun werden die erzeugten Chunks in numerische Vektoren (*Embeddings*) umgewandelt.  \n",
    "Dies erfolgt mithilfe der OpenAI API √ºber `LiteLLM`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29694ebb69205442",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Beispielkonfiguration (OpenAI, austauschbar)\n",
    "litellm_model = \"openai/embedding-3-small\"\n",
    "\n",
    "# Liste von Embeddings vorbereiten\n",
    "embeddings = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    response = litellm.completion( \n",
    "        model=litellm_model,\n",
    "        input=chunk,\n",
    "        api_type=\"embedding\"\n",
    "    )\n",
    "    embeddings.append(response[\"data\"][0][\"embedding\"]) \n",
    "\n",
    "print(f\"{len(embeddings)} Embeddings erstellt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afec90df299ef6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üß† Schritt 4: Erstellung eines Vektorstores\n",
    "\n",
    "Die Embeddings und ihre zugeh√∂rigen Textabschnitte werden in einem einfachen Vektorstore gespeichert.  \n",
    "Dazu erstellen wir eine **Liste von Paaren** bestehend aus:\n",
    "\n",
    "- einem Embedding  \n",
    "- dem zugeh√∂rigen Textabschnitt\n",
    "\n",
    "‚û°Ô∏è Dies erlaubt sp√§ter eine **schnelle semantische Suche**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35abbf069abad08f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Einfache Speicherstruktur f√ºr Embeddings + zugeh√∂rige Chunks ---\n",
    "\n",
    "vectorstore: List[Tuple[List[float], str]] = list(zip(___, ___)) # TODO: Embeddings + zugeh√∂rige Text Chunk definieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1347e831917212a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üîç Schritt 5: Retrieval der relevanten Textabschnitte\n",
    "\n",
    "Um zu einer Nutzeranfrage passende Textstellen zu finden, berechnen wir die **Cosine Similarity**  \n",
    "zwischen dem Embedding der Frage und allen gespeicherten Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93b147ccd74e35",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Cosine Similarity berechnen ---\n",
    "def cosine_similarity(embedding_1: np.ndarray, embedding_2: np.ndarray) -> float:\n",
    "    return np.dot(embedding_1, embedding_2) / (np.linalg.norm(embedding_1) * np.linalg.norm(embedding_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666614c304af9fd0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nun sollen die **k √§hnlichsten Chunks** zur Nutzeranfrage (`query`) gefunden und zur√ºckgegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc00be26937037f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- √Ñhnlichste Chunks zur Nutzerfrage finden ---\n",
    "def retrieve_top_k(query: str, k: int = 3) -> List[str]:\n",
    "    response = litellm.completion(\n",
    "        model=litellm_model,\n",
    "        input=___,      # TODO: Ersetze durch die Nutzeranfrage\n",
    "        api_type=___    # TODO: API-Typ definieren\n",
    "    )\n",
    "    query_embedding = np.array(response[\"data\"][0][\"embedding\"])\n",
    "\n",
    "    # √Ñhnlichkeit mit allen gespeicherten Embeddings berechnen\n",
    "    scored_chunks = []\n",
    "    for embedding, text in vectorstore:\n",
    "        embedding = np.array(embedding)\n",
    "        score = cosine_similarity(___, ___) # TODO: Cosine Similarity korrekt aufrufen\n",
    "        scored_chunks.append((score, text))\n",
    "\n",
    "    # Chunks nach Score absteigend sortieren\n",
    "    scored_chunks = sorted(\n",
    "        ___,                      # TODO: Liste der Scoring-Ergebnisse einsetzen\n",
    "        key=lambda x: x[0],       # Sortiere nach dem ersten Element im Tupel = Score\n",
    "        reverse=True              # H√∂chste Scores zuerst\n",
    "    )\n",
    "\n",
    "    # Texte der Top-k Ergebnisse zur√ºckgeben\n",
    "    top_chunks = []\n",
    "\n",
    "    for i in range(min(___, len(___))):  # TODO: Ersetze beide ___ mit der gew√ºnschten Anzahl an Ergebnissen und der L√§nge der Liste scored_chunks\n",
    "        top_chunks.append(scored_chunks[i][1])  \n",
    "        \n",
    "    return top_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26238a794e42db",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üí¨ Schritt 6: Nutzeranfrage stellen und Antwort generieren\n",
    "\n",
    "Die Nutzerfrage wird zun√§chst ebenfalls in ein Embedding umgewandelt.  \n",
    "Danach werden die semantisch √§hnlichsten Chunks aus dem Vektorstore geladen.  \n",
    "Diese bilden den **Kontext**, den das Sprachmodell (z.B. GPT-4) verwendet, um eine Antwort zu generieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4652a1a28155b2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Nutzerfrage stellen ---\n",
    "query = \"...\"  # TODO: Gib hier deine Frage ein\n",
    "\n",
    "# --- Passende Chunks aus selbstgebauter Vektorstore abfragen ---\n",
    "top_chunks = retrieve_top_k(___, ___)  # TODO: Argumente der Abfragefunktion definieren\n",
    "\n",
    "# --- Prompt vorbereiten ---\n",
    "retrieved_context = \"\\n\\n\".join(top_chunks)\n",
    "\n",
    "# --- Prompt + Frage an Sprachmodell √ºbergeben (z.B. GPT-4 via LiteLLM) ---\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-4\",  # TODO: Modell ggf. anpassen\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Beantworte Fragen basierend auf den folgenden Textausz√ºgen.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Textausz√ºge: {___}\\n\\nFrage: {___}\"} # TODO: √Ñhnliche Textausz√ºge und Nutzteranfrage definieren\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Antwort anzeigen ---\n",
    "print(\"Antwort:\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5251dc6b2ee3c0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In diesem Tutorial hast du Schritt f√ºr Schritt ein einfaches Retrieval-Augmented Generation (RAG) System aufgebaut.\n",
    "\n",
    "Du hast gelernt:\n",
    "\n",
    "- wie man Dokumente in Text umwandelt,\n",
    "- wie man diesen Text in verarbeitbare Chunks aufteilt,\n",
    "- wie man eigene Embeddings erzeugt,\n",
    "- und wie man eine semantische Suche selbst implementiert.\n",
    "\n",
    "Anschlie√üend konntest du mit Hilfe eines Sprachmodells Fragen zu beliebigen Dokumenten beantworten.\n",
    "\n",
    "üîß Dieses Grundger√ºst l√§sst sich nun beliebig erweitern ‚Äì z.B. mit:\n",
    "- Vektor-Datenbanken wie FAISS, Chroma oder Weaviate\n",
    "- lokalen Sprachmodellen (z.B. √ºber Ollama, Hugging Face)\n",
    "- anderen Datenquellen (z.B. HTML, CSV, Notizen, Mails)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
