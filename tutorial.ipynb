{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Einf√ºhrung: Retrieval-Augmented Generation (RAG)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d879f24a0a883238"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dieses Notebook demonstriert die Grundstruktur eines Retrieval-Augmented Generation (RAG) Systems.  \n",
    "Dabei wird ein lokales PDF-Dokument eingelesen, in kleinere Abschnitte (*Chunks*) aufgeteilt, als Vektoren gespeichert und anschlie√üend mit einem Sprachmodell abgefragt.\n",
    "\n",
    "**Ziel:**  \n",
    "Fragen zu benutzerdefinierten Dokumenten beantworten ‚Äì **ohne das Modell neu zu trainieren**.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "927e11155c474a6"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# --- Ben√∂tigten Pakete installieren ---\n",
    "\n",
    "!pip install -q pymupdf        # F√ºr PDF-Text-Extraktion\n",
    "!pip install -q numpy          # F√ºr Cosine Similarity & Vektorberechnungen\n",
    "!pip install -q litellm        # F√ºr Zugriff auf Embedding- & Sprachmodelle via API"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-04T12:35:12.046806Z",
     "start_time": "2025-06-04T12:35:09.887152Z"
    }
   },
   "id": "64eefca87d2f2a9b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import os                 \n",
    "import fitz # PyMuPDF          \n",
    "import numpy as np        \n",
    "import litellm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from typing import List, Tuple"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-04T12:38:03.209129Z",
     "start_time": "2025-06-04T12:38:02.804395Z"
    }
   },
   "id": "518db8ed48a1491"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- OpenAI API Key setzen (√ºber Umgebungsvariable) ---\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # TODO: Ersetze durch deinen eigenen API-Schl√ºssel"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e0fab8996c6118f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üìÑ Schritt 1: PDF-Dokumente einlesen\n",
    "\n",
    "Zuerst werden PDF-Dateien mit dem Python-Paket `fitz` (PyMuPDF) in reinen Text umgewandelt."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c55a13782576ab6e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text from folder './pdfs', showing preview:\n",
      "\n",
      "Polymer Solubility Prediction Using Large\n",
      "Language Models\n",
      "Published as part of ACS Materials Letters special issue ‚ÄúMachine Learning for Materials Chemistry‚Äù.\n",
      "Sakshi Agarwal, Akhlak Mahmood, and Rampi Ramprasad*\n",
      "Cite This: ACS Materials Lett. 2025, 7, 2017‚àí2023\n",
      "Read Online\n",
      "ACCESS\n",
      "Metrics & More\n",
      "Article Recommendations\n",
      "*\n",
      "sƒ±\n",
      "Supporting Information\n",
      "ABSTRACT: Traditional approaches in polymer informatics often require labor-intensive data curation, time-\n",
      "consuming preprocessing such as fingerprinting, and choosing suitable learning algorithms. Large language models\n",
      "(LLMs) represent a compelling alternative by addressing these limitations with their inherent flexibility, ease of use,\n",
      "and scalability. In this study, we propose a novel approach utilizing fine-tuned LLMs to classify solvents and\n",
      "nonsolvents for polymers, a property critical to polymer synthesis, purification, and diverse applications. Our results\n",
      "show that fine-tuned GPT-3.5 achieves predictive performance comparable to or exc\n"
     ]
    }
   ],
   "source": [
    "# --- PDF-Dateien aus Ordner laden & Text extrahieren ---\n",
    "\n",
    "def extract_text_from_pdf_folder(pdf_folder_path):\n",
    "    all_text = \"\"\n",
    "    \n",
    "    # √úber alle PDF-Dateien im Ordner iterieren\n",
    "    for filename in os.listdir(pdf_folder_path):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            file_path = os.path.join(pdf_folder_path, filename)\n",
    "            # TODO: √ñffne die PDF-Datei mit fitz.open(file_path) als doc\n",
    "            for page in doc:\n",
    "                # TODO: Verwende .get_text(), um Text zu extrahieren und zu all_text hinzuzuf√ºgen\n",
    "                pass\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "# --- Aufruf der Extraktion ---\n",
    "pdf_folder = \"...\"  # TODO: Gib den Ordnerpfad mit den PDFs an\n",
    "raw_text = extract_text_from_pdf_folder(pdf_folder)\n",
    "\n",
    "# Vorschau des extrahierten Texts\n",
    "print(raw_text[1000]) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-04T12:35:23.890601Z",
     "start_time": "2025-06-04T12:35:23.746594Z"
    }
   },
   "id": "518d16c7c78334ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ‚úÇÔ∏è Schritt 2: Text in Chunks zerlegen\n",
    "\n",
    "Der extrahierte Text wird nun in kleinere, √ºberlappende Abschnitte (*Chunks*) aufgeteilt.  \n",
    "Diese Einheiten k√∂nnen sp√§ter effizient eingebettet und durchsucht werden.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcdb4f58f9e45d74"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260 chunks created.\n"
     ]
    }
   ],
   "source": [
    "# --- Text in √ºberlappende Chunks aufteilen ---\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=___,       # TODO: W√§hle sinnvolle Chunk-Gr√∂√üe\n",
    "    chunk_overlap=___     # TODO: W√§hle √úberlappung\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text()  # TODO: Gib den zu chunkenden Text ein\n",
    "print(f\"{len(chunks)} Chunks erstellt.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-04T12:36:32.805386Z"
    }
   },
   "id": "c79e01284dfb4ac7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üî¢ Schritt 3: Chunks embedden\n",
    "\n",
    "Nun werden die erzeugten Chunks in numerische Vektoren (*Embeddings*) umgewandelt.  \n",
    "Dies erfolgt mithilfe der OpenAI API √ºber `LiteLLM`.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e57895240eb1851"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Beispielkonfiguration (OpenAI, austauschbar)\n",
    "litellm_model = \"openai/embedding-3-small\"\n",
    "\n",
    "# Liste von Embeddings vorbereiten\n",
    "embeddings = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    response = litellm.completion( \n",
    "        model=litellm_model,\n",
    "        input=chunk,\n",
    "        api_type=\"embedding\"\n",
    "    )\n",
    "    embeddings.append(response[\"data\"][0][\"embedding\"]) \n",
    "\n",
    "print(f\"{len(embeddings)} Embeddings erstellt.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29694ebb69205442"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üß† Schritt 4: Erstellung eines Vektorstores\n",
    "\n",
    "Die Embeddings und ihre zugeh√∂rigen Textabschnitte werden in einem einfachen Vektorstore gespeichert.  \n",
    "Dazu erstellen wir eine **Liste von Paaren** bestehend aus:\n",
    "\n",
    "- einem Embedding  \n",
    "- dem zugeh√∂rigen Textabschnitt\n",
    "\n",
    "‚û°Ô∏è Dies erlaubt sp√§ter eine **schnelle semantische Suche**."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74afec90df299ef6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- Einfache Speicherstruktur f√ºr Embeddings + zugeh√∂rige Chunks ---\n",
    "\n",
    "vectorstore: List[Tuple[List[float], str]] = list(zip(___, ___)) # TODO: Embeddings + zugeh√∂rige Text Chunk definieren"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35abbf069abad08f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üîç Schritt 5: Retrieval der relevanten Textabschnitte\n",
    "\n",
    "Um zu einer Nutzeranfrage passende Textstellen zu finden, berechnen wir die **Cosine Similarity**  \n",
    "zwischen dem Embedding der Frage und allen gespeicherten Embeddings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1347e831917212a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- Cosine Similarity berechnen ---\n",
    "def cosine_similarity(embedding_1: np.ndarray, embedding_2: np.ndarray) -> float:\n",
    "    return np.dot(embedding_1, embedding_2) / (np.linalg.norm(embedding_1) * np.linalg.norm(embedding_2))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f93b147ccd74e35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nun sollen die **k √§hnlichsten Chunks** zur Nutzeranfrage (`query`) gefunden und zur√ºckgegeben werden."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "666614c304af9fd0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- √Ñhnlichste Chunks zur Nutzerfrage finden ---\n",
    "def retrieve_top_k(query: str, k: int = 3) -> List[str]:\n",
    "    response = litellm.completion(\n",
    "        model=litellm_model,\n",
    "        input=___,      # TODO: Ersetze durch die Nutzeranfrage\n",
    "        api_type=___    # TODO: API-Typ definieren\n",
    "    )\n",
    "    query_embedding = np.array(response[\"data\"][0][\"embedding\"])\n",
    "\n",
    "    # √Ñhnlichkeit mit allen gespeicherten Embeddings berechnen\n",
    "    scored_chunks = []\n",
    "    for embedding, text in vectorstore:\n",
    "        embedding = np.array(embedding)\n",
    "        score = cosine_similarity(___, ___) # TODO: Cosine Similarity korrekt aufrufen\n",
    "        scored_chunks.append((score, text))\n",
    "\n",
    "    # Chunks nach Score absteigend sortieren\n",
    "    scored_chunks = sorted(\n",
    "        ___,                      # TODO: Liste der Scoring-Ergebnisse einsetzen\n",
    "        key=lambda x: x[0],       # Sortiere nach dem ersten Element im Tupel = Score\n",
    "        reverse=True              # H√∂chste Scores zuerst\n",
    "    )\n",
    "\n",
    "    # Texte der Top-k Ergebnisse zur√ºckgeben\n",
    "    top_chunks = []\n",
    "\n",
    "    for i in range(min(___, len(___))):  # TODO: Ersetze beide ___ mit der gew√ºnschten Anzahl an Ergebnissen und der L√§nge der Liste scored_chunks\n",
    "        top_chunks.append(scored_chunks[i][1])  \n",
    "        \n",
    "    return top_chunks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfc00be26937037f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üí¨ Schritt 6: Nutzeranfrage stellen und Antwort generieren\n",
    "\n",
    "Die Nutzerfrage wird zun√§chst ebenfalls in ein Embedding umgewandelt.  \n",
    "Danach werden die semantisch √§hnlichsten Chunks aus dem Vektorstore geladen.  \n",
    "Diese bilden den **Kontext**, den das Sprachmodell (z.B. GPT-4) verwendet, um eine Antwort zu generieren."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b26238a794e42db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- Nutzerfrage stellen ---\n",
    "query = \"...\"  # TODO: Gib hier deine Frage ein\n",
    "\n",
    "# --- Passende Chunks aus selbstgebauter Vektorstore abfragen ---\n",
    "top_chunks = retrieve_top_k(___, ___)  # TODO: Argumente der Abfragefunktion definieren\n",
    "\n",
    "# --- Prompt vorbereiten ---\n",
    "retrieved_context = \"\\n\\n\".join(top_chunks)\n",
    "\n",
    "# --- Prompt + Frage an Sprachmodell √ºbergeben (z.B. GPT-4 via LiteLLM) ---\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-4\",  # TODO: Modell ggf. anpassen\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Beantworte Fragen basierend auf den folgenden Textausz√ºgen.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Textausz√ºge: {___}\\n\\nFrage: {___}\"} # TODO: √Ñhnliche Textausz√ºge und Nutzteranfrage definieren\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Antwort anzeigen ---\n",
    "print(\"Antwort:\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb4652a1a28155b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In diesem Tutorial hast du Schritt f√ºr Schritt ein einfaches Retrieval-Augmented Generation (RAG) System aufgebaut.\n",
    "\n",
    "Du hast gelernt:\n",
    "\n",
    "- wie man Dokumente in Text umwandelt,\n",
    "- wie man diesen Text in verarbeitbare Chunks aufteilt,\n",
    "- wie man eigene Embeddings erzeugt,\n",
    "- und wie man eine semantische Suche selbst implementiert.\n",
    "\n",
    "Anschlie√üend konntest du mit Hilfe eines Sprachmodells Fragen zu beliebigen Dokumenten beantworten.\n",
    "\n",
    "üîß Dieses Grundger√ºst l√§sst sich nun beliebig erweitern ‚Äì z.B. mit:\n",
    "- Vektor-Datenbanken wie FAISS, Chroma oder Weaviate\n",
    "- lokalen Sprachmodellen (z.B. √ºber Ollama, Hugging Face)\n",
    "- anderen Datenquellen (z.B. HTML, CSV, Notizen, Mails)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac5251dc6b2ee3c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
