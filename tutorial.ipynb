{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d879f24a0a883238",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Einf√ºhrung: Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e11155c474a6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Dieses Notebook demonstriert die Grundstruktur eines Retrieval-Augmented Generation (RAG) Systems.  \n",
    "RAG kombiniert die St√§rken von Retrieval- und Generierungsmodellen, um pr√§zise Antworten auf Fragen zu liefern, die auf spezifischen Dokumenten basieren.\n",
    "Retrieval-Modelle durchsuchen gro√üe Dokumentensammlungen, um relevante Informationen zu finden, w√§hrend Generierungsmodelle diese Informationen nutzen, um koh√§rente Antworten zu formulieren.\n",
    "\n",
    "Praktisch kann ein solches System genutzt werden, um Fragen zu beantworten, die auf einem bestimmten Dokument basieren, ohne dass das zugrunde liegende Sprachmodell neu trainiert werden muss.\n",
    "\n",
    "## Aufbau eines RAG-Systems\n",
    "Prinzipiell besteht ein RAG-System aus zwei Hauptkomponenten:\n",
    "1. **Retrieval**: Hierbei werden relevante Abschnitte aus einem Dokument oder einer Sammlung von Dokumenten abgerufen, die f√ºr die Beantwortung der gestellten Frage n√ºtzlich sein k√∂nnten. Hierf√ºr werden Dokumente in kleine Abschnitte (\"Chunks\") unterteilt und in einem Vektor-Datenbankindex gespeichert.  Der Index wird erstellt, indem die Abschnitte in Vektoren umgewandelt werden, die dann in der Datenbank gespeichert werden. Wenn eine Frage gestellt wird, wird der Index durchsucht, um die relevantesten Abschnitte zu finden.\n",
    "2. **Generierung**: Basierend auf den abgerufenen Informationen generiert ein Sprachmodell eine Antwort auf die gestellte Frage.\n",
    "\n",
    "\n",
    "## Ziel des Notebooks \n",
    "Fragen zu benutzerdefinierten Dokumenten beantworten ‚Äì **ohne das Modell neu zu trainieren**.\n",
    "\n",
    "## Voraussetzungen\n",
    "- Python 3.8 oder h√∂her\n",
    "- Ein Ordner mit PDF-Dokumenten, die Sie verwenden m√∂chten (z.B. `./data`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eefca87d2f2a9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:35:12.046806Z",
     "start_time": "2025-06-04T12:35:09.887152Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Ben√∂tigten Pakete installieren ---\n",
    "\n",
    "!pip install -q pymupdf        # F√ºr PDF-Text-Extraktion\n",
    "!pip install -q numpy          # F√ºr Cosine Similarity & Vektorberechnungen\n",
    "!pip install -q litellm        # F√ºr Zugriff auf Embedding- & Sprachmodelle via API\n",
    "!pip install -q langchain      # F√ºr die Verwaltung von Embeddings und Modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518db8ed48a1491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:38:03.209129Z",
     "start_time": "2025-06-04T12:38:02.804395Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import os                 \n",
    "import fitz # PyMuPDF          \n",
    "import numpy as np        \n",
    "import litellm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from typing import List, Tuple\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219db1c",
   "metadata": {},
   "source": [
    "Zunachst laden wir die Umgebungsvariablen aus der `.env`-Datei, die API-Schl√ºssel und andere Konfigurationen enthalten sollte.\n",
    "Es ist zu empfehlen, dass solche API-Schl√ºssel nicht direkt im Code stehen, sondern in einer `.env`-Datei gespeichert werden.\n",
    "\n",
    "Dafur erstellen sie im gleichen Verzeichnis wie dieses Skript eine Datei mit dem Namen `.env` und f√ºgen dort Ihre API-Schl√ºssel ein, z.B.:\n",
    "```plaintext\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "```\n",
    "\n",
    "Alternativ kann auch Groq oder ein anderer Provider verwendet werden. Eine komplette √úbersicht gibt es auf https://docs.litellm.ai/docs/providers.  \n",
    "Cohere bietet kostenlose API Keys mit einer Token-Begrenzung an https://docs.cohere.com/v2/docs/rate-limits \n",
    "\n",
    "`load_dotenv()` l√§dt die Umgebungsvariablen aus der `.env`-Datei, sodass wir sie im Code verwenden k√∂nnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ad53a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()  # L√§dt Umgebungsvariablen aus .env-Datei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a13782576ab6e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üìÑ Schritt 1: PDF-Dokumente einlesen\n",
    "\n",
    "Zuerst werden PDF-Dateien mit dem Python-Paket `fitz` (PyMuPDF) in reinen Text umgewandelt. Daf√ºr definieren wir eine Funktion `extract_text_from_pdfs`, die alle PDF-Dateien in einem angegebenen Verzeichnis liest und den Text extrahiert.\n",
    "Die Funktion ist unvollst√§ndig und muss noch implementiert werden.  Hinweise zum Implementieren der Funktion finden Sie in den Kommentaren im Code sowie in der [Dokumentation von `fitz` (PyMuPDF)](https://pymupdf.readthedocs.io/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d16c7c78334ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:35:23.890601Z",
     "start_time": "2025-06-04T12:35:23.746594Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf_folder(pdf_folder_path):\n",
    "    all_text = \"\"\n",
    "    \n",
    "    # √úber alle PDF-Dateien im Ordner iterieren\n",
    "    for filename in os.listdir(pdf_folder_path):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            file_path = os.path.join(pdf_folder_path, filename)\n",
    "            # TODO: √ñffne die PDF-Datei mit fitz.open(file_path) als doc\n",
    "            for page in doc:\n",
    "                # TODO: Verwende .get_text(), um Text zu extrahieren und zu all_text hinzuzuf√ºgen\n",
    "                pass\n",
    "    \n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c7563",
   "metadata": {},
   "source": [
    "Nun k√∂nnen wir die Funktion zum Extrahieren von Text aus PDF-Dateien testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40221fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aufruf der Extraktion ---\n",
    "pdf_folder = \"...\"  # TODO: Gib den Ordnerpfad mit den PDFs an\n",
    "raw_text = extract_text_from_pdf_folder(pdf_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137fac1b",
   "metadata": {},
   "source": [
    "Und uns den extrahierten Text anzeigen lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_text[1000]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb4f58f9e45d74",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ‚úÇÔ∏è Schritt 2: Text in Chunks zerlegen\n",
    "\n",
    "Der extrahierte Text wird nun in kleinere, √ºberlappende Abschnitte (*Chunks*) aufgeteilt.  \n",
    "Hierfur verwenden wir eine Methode die `RecursiveCharacterTextSplitter` genannt wird. Diese Methode teilt den Text in kleinere Abschnitte auf, die f√ºr die sp√§tere Verarbeitung durch das Retrieval-Modell geeignet sind. Die Chunks werden so erstellt, dass sie eine maximale L√§nge haben und √ºberlappende Teile enthalten, um sicherzustellen, dass wichtige Informationen nicht verloren gehen. Die Abschnitte werden hierbei erstellt indem der Text an einer definierten Liste von Trennzeichen (wie Abs√§tzen oder S√§tzen) aufgeteilt wird. Diese Liste wird durchgegangen bis der Text in kleinere Abschnitte zerlegt ist, die eine maximale L√§nge nicht √ºberschreiten.\n",
    "Mehr Informationen dazu k√∂nnen in der [LangChain Dokumentation](https://python.langchain.com/docs/how_to/recursive_text_splitter/) gefunden werden.\n",
    "\n",
    "Diese Einheiten k√∂nnen sp√§ter effizient eingebettet und durchsucht werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e01284dfb4ac7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-04T12:36:32.805386Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Text in √ºberlappende Chunks aufteilen ---\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=___,       # TODO: W√§hle sinnvolle Chunk-Gr√∂√üe\n",
    "    chunk_overlap=___     # TODO: W√§hle √úberlappung\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text()  # TODO: Gib den zu chunkenden Text ein\n",
    "print(f\"{len(chunks)} Chunks erstellt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57895240eb1851",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üî¢ Schritt 3: Chunks embedden\n",
    "\n",
    "Nun werden die erzeugten Chunks in numerische Vektoren (*Embeddings*) umgewandelt.  \n",
    "Diese Vektoren repr√§sentieren die semantische Bedeutung der Chunks und erm√∂glichen es, √§hnliche Chunks zu finden. Texte mit √§hnlicher Bedeutung werden in der Vektor-Datenbank nahe beieinander liegen. \n",
    "Text in Vektoren umzuwandeln, wird als \"Embedding\" bezeichnet und ist der erste Schritt in Sprachmodellen, um Text in eine Form zu bringen, die von Computern verarbeitet werden kann. Hier verwenden wir die Embeddings allerdings auch um die Chunks in einer Vektor-Datenbank zu speichern, damit sie sp√§ter f√ºr die Retrieval-Komponente des RAG-Systems verwendet werden k√∂nnen.\n",
    "\n",
    "\n",
    "Dies erfolgt mithilfe der OpenAI API √ºber `LiteLLM`. Es k√∂nnen auch andere Embedding-Modelle verwendet werden, die in der Lage sind, Text in Vektoren umzuwandeln. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e645a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_with_litellm(text: str, model: str = 'text-embedding-3-small') -> List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the given text using the LiteLLM API.\n",
    "    \"\"\"\n",
    "    response = litellm.embedding(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "    return response[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f75b71",
   "metadata": {},
   "source": [
    "Das k√∂nnen wir testen indem wir verschiedene Texte einbetten und die Vektoren vergleichen. Um  Vektoren zu vergleichen, k√∂nnen wir den Winkel zwischen ihnen berechnen. Ein kleiner Winkel bedeutet, dass die Vektoren √§hnlich sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e626fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Berechnet die Cosine Similarity zwischen zwei Vektoren.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_a = np.linalg.norm(vec1)\n",
    "    norm_b = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6eddbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a = \"Chemie\"\n",
    "text_b = \"Chemie ist die Wissenschaft von Stoffen und deren Umwandlungen.\"\n",
    "text_c = \"Mathematik ist die Wissenschaft von Zahlen und Formen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cabb4970",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_a_b = cosine_similarity(\n",
    "    np.array(embed_text_with_litellm(text_a)),\n",
    "    np.array(embed_text_with_litellm(text_b))\n",
    ")\n",
    "\n",
    "cosine_a_c = cosine_similarity(\n",
    "    np.array(embed_text_with_litellm(text_a)),\n",
    "    np.array(embed_text_with_litellm(text_c))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b85c8350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity zwischen 'Chemie' und 'Chemie ist die Wissenschaft von Stoffen und deren Umwandlungen.': 0.7127\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cosine Similarity zwischen '{text_a}' und '{text_b}': {cosine_a_b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb8307c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity zwischen 'Chemie' und 'Mathematik ist die Wissenschaft von Zahlen und Formen.': 0.3313\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cosine Similarity zwischen '{text_a}' und '{text_c}': {cosine_a_c:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae3d7e",
   "metadata": {},
   "source": [
    "Nun k√∂nnen wir die Chunks in Embeddings umwandeln und die Cosine Similarity berechnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29694ebb69205442",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Beispielkonfiguration (OpenAI, austauschbar)\n",
    "litellm_model = \"openai/embedding-3-small\"\n",
    "\n",
    "# Liste von Embeddings vorbereiten\n",
    "embeddings = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    embeddings.append(embed_text_with_litellm(chunk, model=litellm_model))\n",
    "\n",
    "print(f\"{len(embeddings)} Embeddings erstellt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41526b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74afec90df299ef6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üß† Schritt 4: Erstellung eines Vektorstores\n",
    "\n",
    "Die Embeddings und ihre zugeh√∂rigen Textabschnitte werden in einem einfachen Vektorstore gespeichert.  \n",
    "Dazu erstellen wir eine **Liste von Paaren** bestehend aus:\n",
    "\n",
    "- einem Embedding  \n",
    "- dem zugeh√∂rigen Textabschnitt\n",
    "\n",
    "‚û°Ô∏è Dies erlaubt sp√§ter eine **schnelle semantische Suche**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35abbf069abad08f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Einfache Speicherstruktur f√ºr Embeddings + zugeh√∂rige Chunks ---\n",
    "\n",
    "vectorstore: List[Tuple[List[float], str]] = list(zip(___, ___)) # TODO: Embeddings + zugeh√∂rige Text Chunk definieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1347e831917212a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üîç Schritt 5: Retrieval der relevanten Textabschnitte\n",
    "\n",
    "Um zu einer Nutzeranfrage passende Textstellen zu finden, berechnen wir die **Cosine Similarity**  \n",
    "zwischen dem Embedding der Frage und allen gespeicherten Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93b147ccd74e35",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Cosine Similarity berechnen ---\n",
    "def cosine_similarity(embedding_1: np.ndarray, embedding_2: np.ndarray) -> float:\n",
    "    return np.dot(embedding_1, embedding_2) / (np.linalg.norm(embedding_1) * np.linalg.norm(embedding_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666614c304af9fd0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nun sollen die **k √§hnlichsten Chunks** zur Nutzeranfrage (`query`) gefunden und zur√ºckgegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc00be26937037f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- √Ñhnlichste Chunks zur Nutzerfrage finden ---\n",
    "def retrieve_top_k(query: str, k: int = 3) -> List[str]:\n",
    "    response = litellm.completion(\n",
    "        model=litellm_model,\n",
    "        input=___,      # TODO: Ersetze durch die Nutzeranfrage\n",
    "        api_type=___    # TODO: API-Typ definieren\n",
    "    )\n",
    "    query_embedding = np.array(response[\"data\"][0][\"embedding\"])\n",
    "\n",
    "    # √Ñhnlichkeit mit allen gespeicherten Embeddings berechnen\n",
    "    scored_chunks = []\n",
    "    for embedding, text in vectorstore:\n",
    "        embedding = np.array(embedding)\n",
    "        score = cosine_similarity(___, ___) # TODO: Cosine Similarity korrekt aufrufen\n",
    "        scored_chunks.append((score, text))\n",
    "\n",
    "    # Chunks nach Score absteigend sortieren\n",
    "    scored_chunks = sorted(\n",
    "        ___,                      # TODO: Liste der Scoring-Ergebnisse einsetzen\n",
    "        key=lambda x: x[0],       # Sortiere nach dem ersten Element im Tupel = Score\n",
    "        reverse=True              # H√∂chste Scores zuerst\n",
    "    )\n",
    "\n",
    "    # Texte der Top-k Ergebnisse zur√ºckgeben\n",
    "    top_chunks = []\n",
    "\n",
    "    for i in range(min(___, len(___))):  # TODO: Ersetze beide ___ mit der gew√ºnschten Anzahl an Ergebnissen und der L√§nge der Liste scored_chunks\n",
    "        top_chunks.append(scored_chunks[i][1])  \n",
    "        \n",
    "    return top_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26238a794e42db",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## üí¨ Schritt 6: Nutzeranfrage stellen und Antwort generieren\n",
    "\n",
    "Die Nutzerfrage wird zun√§chst ebenfalls in ein Embedding umgewandelt.  \n",
    "Danach werden die semantisch √§hnlichsten Chunks aus dem Vektorstore geladen.  \n",
    "Diese bilden den **Kontext**, den das Sprachmodell (z.B. GPT-4) verwendet, um eine Antwort zu generieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4652a1a28155b2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Nutzerfrage stellen ---\n",
    "query = \"...\"  # TODO: Gib hier deine Frage ein\n",
    "\n",
    "# --- Passende Chunks aus selbstgebauter Vektorstore abfragen ---\n",
    "top_chunks = retrieve_top_k(___, ___)  # TODO: Argumente der Abfragefunktion definieren\n",
    "\n",
    "# --- Prompt vorbereiten ---\n",
    "retrieved_context = \"\\n\\n\".join(top_chunks)\n",
    "\n",
    "# --- Prompt + Frage an Sprachmodell √ºbergeben (z.B. GPT-4 via LiteLLM) ---\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-4\",  # TODO: Modell ggf. anpassen\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Beantworte Fragen basierend auf den folgenden Textausz√ºgen.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Textausz√ºge: {___}\\n\\nFrage: {___}\"} # TODO: √Ñhnliche Textausz√ºge und Nutzteranfrage definieren\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Antwort anzeigen ---\n",
    "print(\"Antwort:\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5251dc6b2ee3c0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In diesem Tutorial hast du Schritt f√ºr Schritt ein einfaches Retrieval-Augmented Generation (RAG) System aufgebaut.\n",
    "\n",
    "Du hast gelernt:\n",
    "\n",
    "- wie man Dokumente in Text umwandelt,\n",
    "- wie man diesen Text in verarbeitbare Chunks aufteilt,\n",
    "- wie man eigene Embeddings erzeugt,\n",
    "- und wie man eine semantische Suche selbst implementiert.\n",
    "\n",
    "Anschlie√üend konntest du mit Hilfe eines Sprachmodells Fragen zu beliebigen Dokumenten beantworten.\n",
    "\n",
    "üîß Dieses Grundger√ºst l√§sst sich nun beliebig erweitern ‚Äì z.B. mit:\n",
    "- Vektor-Datenbanken wie FAISS, Chroma oder Weaviate\n",
    "- lokalen Sprachmodellen (z.B. √ºber Ollama, Hugging Face)\n",
    "- anderen Datenquellen (z.B. HTML, CSV, Notizen, Mails)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
