{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RAG Hands-on Tutorial"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d879f24a0a883238"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dieses Notebook demonstriert die Grundstruktur eines Retrieval-Augmented Generation (RAG) Systems. Dabei wird ein lokales Dokument (z. B. eine PDF-Datei) hochgeladen, in kleinere Abschnitte aufgeteilt, in eine Vektordatenbank eingebettet und anschließend mithilfe eines Sprachmodells abgefragt.\n",
    "\n",
    "Das Ziel ist es, Fragen zu benutzerdefinierten Dokumenten zu beantworten, ohne das Modell neu trainieren zu müssen."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "927e11155c474a6"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Installieren aller nötigen packages\n",
    "!pip install -q pymupdf langchain faiss-cpu litellm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-04T12:35:12.046806Z",
     "start_time": "2025-06-04T12:35:09.887152Z"
    }
   },
   "id": "64eefca87d2f2a9b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# --- Imports & API-Key-Setup ---\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import tempfile\n",
    "import litellm\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-04T12:38:03.209129Z",
     "start_time": "2025-06-04T12:38:02.804395Z"
    }
   },
   "id": "518db8ed48a1491"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Definieren des OpenAI API Keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e0fab8996c6118f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Embedden der Paper\n",
    "\n",
    "Zuerst werden wir nun die PDF Dokumente mittels des Python packages fitz in Text umwandeln."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c55a13782576ab6e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text from folder './pdfs', showing preview:\n",
      "\n",
      "Polymer Solubility Prediction Using Large\n",
      "Language Models\n",
      "Published as part of ACS Materials Letters special issue “Machine Learning for Materials Chemistry”.\n",
      "Sakshi Agarwal, Akhlak Mahmood, and Rampi Ramprasad*\n",
      "Cite This: ACS Materials Lett. 2025, 7, 2017−2023\n",
      "Read Online\n",
      "ACCESS\n",
      "Metrics & More\n",
      "Article Recommendations\n",
      "*\n",
      "sı\n",
      "Supporting Information\n",
      "ABSTRACT: Traditional approaches in polymer informatics often require labor-intensive data curation, time-\n",
      "consuming preprocessing such as fingerprinting, and choosing suitable learning algorithms. Large language models\n",
      "(LLMs) represent a compelling alternative by addressing these limitations with their inherent flexibility, ease of use,\n",
      "and scalability. In this study, we propose a novel approach utilizing fine-tuned LLMs to classify solvents and\n",
      "nonsolvents for polymers, a property critical to polymer synthesis, purification, and diverse applications. Our results\n",
      "show that fine-tuned GPT-3.5 achieves predictive performance comparable to or exc\n"
     ]
    }
   ],
   "source": [
    "# --- Aufgabe: Lade alle PDF-Dateien aus einem Ordner und extrahiere den Text ---\n",
    "\n",
    "def extract_text_from_pdf_folder(pdf_folder_path):\n",
    "    all_text = \"\"\n",
    "    \n",
    "    # TODO: Iteriere über alle Dateien im angegebenen Ordner\n",
    "        # TODO: Verarbeite nur .pdf-Dateien\n",
    "        # TODO: Öffne die PDF-Datei mit fitz.open()\n",
    "        # TODO: Iteriere über alle Seiten und extrahiere den Text mit .get_text()\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "# --- Aufruf ---\n",
    "pdf_folder = \"...\"  # TODO: Ordnerpfad angeben\n",
    "raw_text = extract_text_from_pdf_folder(pdf_folder)\n",
    "\n",
    "# Vorschau des extrahierten Texts\n",
    "print(raw_text[1000]) \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-04T12:35:23.890601Z",
     "start_time": "2025-06-04T12:35:23.746594Z"
    }
   },
   "id": "518d16c7c78334ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Als nächstes wird nun der Text in kleinere Abschnitte (engl. Chunks) aufgeteilt. Dazu müssen passende Parameter für die Länge der Chunks sowie für die Überlappung dieser gewählt werden. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcdb4f58f9e45d74"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260 chunks created.\n"
     ]
    }
   ],
   "source": [
    "# --- Aufgabe: Text in überlappende Chunks aufteilen ---\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=___,       # TODO: Wähle sinnvolle Chunk-Größe\n",
    "    chunk_overlap=___     # TODO: Bestimme Überlappung\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text()  # TODO: Wähle Eingabetext\n",
    "print(f\"{len(chunks)} chunks created.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-04T12:36:32.805386Z"
    }
   },
   "id": "c79e01284dfb4ac7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nun müssen alle erstellten Chunks in einen Vektor (sogenannte Embedding) umgewandlet werden. Die Embeddings werden mit der OpenAI API erstellt. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e57895240eb1851"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Beispielkonfiguration (OpenAI, austauschbar)\n",
    "litellm_model = \"openai/embedding-3-small\"\n",
    "\n",
    "# Liste von Embeddings vorbereiten\n",
    "embeddings = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    response = litellm.completion( \n",
    "        model=litellm_model,\n",
    "        input=chunk,\n",
    "        api_type=\"embedding\"\n",
    "    )\n",
    "    embeddings.append(response[\"data\"][0][\"embedding\"]) \n",
    "\n",
    "print(f\"{len(embeddings)} Embeddings erstellt.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29694ebb69205442"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementation des Vektorstores\n",
    "\n",
    "Nun müssen die Embeddings der einzelnen Cgunks in einem Vektortore gespeichert werden. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74afec90df299ef6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
